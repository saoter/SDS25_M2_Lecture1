{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment predictions using Bag-of-Words features\n",
    "\n",
    "We will first import packages, download dataset, conduct BoW, and at the end use logistic regression. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset # Huggingface package for downloading datasets\n",
    "from sklearn.feature_extraction.text import CountVectorizer # Package for BoW\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the IMDB dataset. Go to https://huggingface.co/datasets for more datasets\n",
    "imdb_dataset = load_dataset(\"imdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect dataset structure \n",
    "imdb_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check the first datapoint in train part\n",
    "imdb_dataset['train'][[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training set consist of 'text' which is users' comment and 'label'. The later in binary and it has value 0 (negative sentiment) and 1 (positive sentiment)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets. You learned this in previous module!\n",
    "train_data = imdb_dataset['train']\n",
    "test_data = imdb_dataset['test']\n",
    "\n",
    "# Extract the text reviews and their labels\n",
    "train_reviews = train_data['text']\n",
    "\n",
    "train_labels = train_data['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Bag-of-Words model using CountVectorizer\n",
    "vectorizer = CountVectorizer(max_features=5000, stop_words='english')  # Limit to 5000 features and remove English stop words\n",
    "\n",
    "# Fit the vectorizer on the training data and transform the reviews into BoW vectors\n",
    "X_train_bow = vectorizer.fit_transform(train_reviews)\n",
    "\n",
    "# Convert the BoW to a DataFrame \n",
    "bow_df = pd.DataFrame(X_train_bow.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "# Display the first few rows of the Bag-of-Words matrix\n",
    "bow_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see how each row consist of 0 and 1 as a big sparse matrix. In a way we can say each review is represented as the vector of lenght 5000 (number of words used in BoW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the Bag-of-Words features in a machine learning model\n",
    "# We will use logistic regression as a classifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Split training data for validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_bow, train_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the classifier and train it on the BoW vectors\n",
    "clf = LogisticRegression(max_iter=1000)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the classifier on the validation set\n",
    "y_pred = clf.predict(X_val)\n",
    "accuracy = accuracy_score(y_val, y_pred)\n",
    "print(f\"Validation Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BoW is very simple and easy approach. In our example it reaches very good accuracy. But there are some drawbacks of this approach as well. Due to the simplicity its usage is very limited. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SDS24",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
