{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic modeling (academic articles)\n",
    "\n",
    "For this exercises I downloaded information about all published academic articles with keyword **natural language processing** from Web Of Scinece database. You have access to it via AAU Library. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import textwrap\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "from tabulate import tabulate\n",
    "from prettytable import PrettyTable\n",
    "from pyBibX.base import pbx_probe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load .bib\n",
    "# Arguments: file_bib = 'filename.bib'; db = 'scopus', 'wos', 'pubmed'; del_duplicated = True, False\n",
    "file_name = './data/nlp.bib'\n",
    "database  = 'wos'\n",
    "bibfile   = pbx_probe(file_bib = file_name, db = database, del_duplicated = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tabulate(bibfile.data.head(n = 10), headers = 'keys', tablefmt = 'psql'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the contents of 'table_id_kwa' if it's related to author keywords\n",
    "print(bibfile.table_id_kwa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WordCloud from the Abstracts\n",
    "bibfile.word_cloud_plot(entry = 'abs', size_x = 15, size_y = 10, wordsn = 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Table\n",
    "table             = PrettyTable()\n",
    "data_wd           = bibfile.ask_gpt_wd\n",
    "table.field_names = ['Word', 'Importance']\n",
    "for key, value in data_wd.items():\n",
    "    table.add_row([key, round(value, 4)])\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N-Grams\n",
    "bibfile.get_top_ngrams(view = 'notebook', entry = 'kwa', ngrams = 3, stop_words = [], rmv_custom_words = [], wordsn = 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Table\n",
    "data_ng = bibfile.ask_gpt_ng\n",
    "display(HTML(data_ng.head(15).to_html()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Documents Projection based on Words\n",
    "projection, labels = bibfile.docs_projection(view              = 'notebook',\n",
    "                                             corpus_type       = 'abs',\n",
    "                                             stop_words        = ['en'],\n",
    "                                             rmv_custom_words  = [],\n",
    "                                             custom_label      = [],\n",
    "                                             custom_projection = [],\n",
    "                                             n_components      = 2,\n",
    "                                             n_clusters        = 5,\n",
    "                                             tf_idf            = False,\n",
    "                                             embeddings        = False,\n",
    "                                             method            = 'umap')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLP\n",
    "# Arguments: corpus_type       = 'abs', 'title', 'kwa', or 'kwp';\n",
    "#            stop_words        = A list of stopwords to clean the corpus. ['ar', 'bn', 'bg', 'cs', 'en', 'fi', 'fr', 'de', 'el', 'hi', 'he', 'hu', 'it', 'ja', 'ko',  'mr', 'fa', 'pl', 'pt-br', 'ro', 'ru', 'es', 'sv', 'sk', 'zh', 'th', 'uk'];\n",
    "#                                'ar' = Arabic; 'bn' = Bengali; 'bg' = Bulgarian; 'cs' = Czech; 'en' = English; 'fi' = Finnish; 'fr' = French; 'de' = German; 'el' = Greek; 'he' = Hebrew;'hi' = Hindi; 'hu' = Hungarian; 'it' = Italian;\n",
    "#                                'ja' = Japanese; 'ko' = Korean; 'mr' =  Marathi; 'fa' =  Persian; 'pl' =  Polish; 'pt-br' = Potuguese-Brazilian; 'ro' = Romanian; 'ru' = Russian; 'es' =  Spanish; 'sk' = Slovak; 'sv' = Swedish;\n",
    "#                                'zh' = Chinese; 'th' = Thai; 'uk' = Ukrainian\n",
    "#            rmv_custom_words  = A list of custom stopwords to clean the corpus;\n",
    "bibfile.create_embeddings(stop_words = ['en'], rmv_custom_words = [], corpus_type = 'abs')\n",
    "emb = bibfile.embds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLP #-1 refers to all outliers and should typically be ignored.\n",
    "# Arguments: stop_words  = A list of stopwords to clean the corpus. ['ar', 'bn', 'bg', 'cs', 'en', 'fi', 'fr', 'de', 'el', 'hi', 'he', 'hu', 'it', 'ja', 'ko',  'mr', 'fa', 'pl', 'pt-br', 'ro', 'ru', 'es', 'sv', 'sk', 'zh', 'th', 'uk'];\n",
    "#                        'ar' = Arabic; 'bn' = Bengali; 'bg' = Bulgarian; 'cs' = Czech; 'en' = English; 'fi' = Finnish; 'fr' = French; 'de' = German; 'el' = Greek; 'he' = Hebrew;'hi' = Hindi; 'hu' = Hungarian; 'it' = Italian;\n",
    "#                        'ja' = Japanese; 'ko' = Korean; 'mr' =  Marathi; 'fa' =  Persian; 'pl' =  Polish; 'pt-br' = Potuguese-Brazilian; 'ro' = Romanian; 'ru' = Russian; 'es' =  Spanish; 'sk' = Slovak; 'sv' = Swedish;\n",
    "#                        'zh' = Chinese; 'th' = Thai; 'uk' = Ukrainian\n",
    "#            rmv_custom_words  = A list of custom stopwords to clean the corpus;\n",
    "#            embeddings        = True or False. If True then word embeddings are used to create the topics\n",
    "bibfile.topics_creation(stop_words = ['en'], rmv_custom_words = [], embeddings = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLP\n",
    "# Each document Topic\n",
    "topics = bibfile.topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLP\n",
    "# Each document Probability to belong a Topic\n",
    "probs = bibfile.probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLP\n",
    "# Arguments: view = 'notebook', 'browser' ('notebook' -> To plot in your prefered Notebook App. 'browser' -> To plot in your prefered browser window)\n",
    "bibfile.graph_topics_distribution(view = 'notebook')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLP\n",
    "# Arguments: view = 'notebook', 'browser' ('notebook' -> To plot in your prefered Notebook App. 'browser' -> To plot in your prefered browser window)\n",
    "bibfile.graph_topics(view = 'notebook')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLP\n",
    "bibfile.graph_topics_projection(view = 'notebook')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLP\n",
    "bibfile.graph_topics_heatmap(view = 'notebook')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLP\n",
    "similar_topics, similarity = bibfile.topic_model.find_topics('word2vec', top_n = 10)\n",
    "for i in range(0, len(similar_topics)):\n",
    "  print('Topic: ', similar_topics[i], 'Correlation: ', round(similarity[i], 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SDS24",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
